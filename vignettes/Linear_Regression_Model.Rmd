---
title: "Multiple Linear Regression Module"
author: "Chandika Nishshanka & Ayesha Gamage"
data: "'r.sys.date()'"
output: 
  rmarkdown::html_vignette:
    fig_width: 10 
    fig_height: 6
vignette: >
  %\VignetteIndexEntry{Multiple Linear Regression Module}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(LinearRegressionModel)
```

# Introduction

This R package will creating for handling linear regression models. The package will draw upon the principles of linear algebra to establish the foundational functionality. Also implement an object oriented system to handle special functions such as print(), plot(), resid(), pred(), coef() and summary().The package can be used to conduct a simple regression analysis using a dateset included in the package.

As we explore the package's design and development, we'll come across different design options, each with varying levels of complexity. These options encompass:

Implementing Calculations Using Ordinary Linear Algebra: This approach forms the core of our package, allowing us to perform regression calculations efficiently.


Implementing the Results as an S3 Class: We'll explore the advantages of creating an S3 class for managing regression results, enhancing the package's structure and usability.


(*) Implementing a 'theme()' for the Graphical Profile: For those who wish to tailor the package's graphical appearance to their preferences, we'll discuss the implementation of a 'theme()' function, adding a layer of customization.

# Background Information:

Write the R Code: In this R package, our primary task is to create a function called linreg() for multiple regression models. This function will accept two arguments: formula and data. The function will return an object of class linreg, either as an S3 class or an RC class. The formula argument will accept a formula object. Within the function, we will begin by using the model.matrix() function to construct the matrix X (representing independent variables) and extract the dependent variable y using all.vars().

Calculate the Following Parameters: Our next step involves calculating essential parameters for the regression model

Regression Coefficients:$$β ̂=(X^T X)^-1 X^T y \tag{1}$$  

Fitted Values: $$y ̂=Xβ ̂ \tag{2}$$                     

Residuals:  $$e ̂=y-y ̂=y-Xβ \tag{3}$$                       

Degrees of Freedom:   $$df=n-p \tag{4}$$                    

Residual Variance:  $$σ ̂^2=(e^T e)/df \tag{5}$$                   

Variance of the Regression Coefficients:  $$Var(β ̂ )=σ ̂^2 (X^T X)^(-1) \tag{6}$$    

t-values for Each Coefficient:		$$t_β=β ̂/√(Var(β ̂ ) ) \tag{7}$$		 

We will utilize the pt() function to calculate the p-values for each regression coefficient. All these statistics will be stored in an object of class linreg. We will also document the linreg() function using roxygen2.

(*) Using the QR Decomposition: In the realm of statistical software, the QR decomposition offers a faster and more robust approach to estimation. We'll explore the computation of `^β` and `Var(^β)`   using a QR decomposition of X.

Implementing Methods for Your Class: Having performed the necessary calculations and stored them in an object with the class linreg, we proceed to implement various "methods" for our object. These implementations can be done using either the S3 or (*) the RC object-oriented system. The following methods will be developed and documented using roxygen2:

This method will print out the coefficients and coefficient names, mirroring the output produced by the lm class.


plot(): Utilizing ggplot2, this method will generate two plots. It's important to ensure that ggplot2 is included in your package. Consider incorporating the median value for added context.

resid(): This method will return the vector of residuals, denoted as `^e`

pred(): This method will provide the predicted values, represented as `^y`.

coef(): This method will return the coefficients as a named vector.`^β`

summary(): This method will furnish a printout similar to that of lm objects, presenting coefficients with their standard error, t-value, p-value, as well as the estimate of  and the degrees of freedom in the model.

Stay tuned as we embark on this journey to create an efficient and user-friendly R package for handling linear regression models.

### Import Sample Dataset iris
```{r}
data(iris)
```

### create a linreg mode object using linreg@new function
```{r}
linreg_mod_object <- linreg$new(Petal.Length~Species, data = iris)
```

### print the coefficients and coefficient names according to multiple regression model
```{r}
linreg_mod_object$print()
```

### print two plots: Residuals vs Fitted and Scale−Location 
```{r}
linreg_mod_object$plot()
```

### print residuals e^ 
```{r}
linreg_mod_object$resid()
```

### print predicted values y^
```{r}
linreg_mod_object$pred()
```

### print the coefficients
```{r}
linreg_mod_object$coef()
```

### print summary statistics
```{r}
linreg_mod_object$summary()
```


# Ridge regression

Add a new function in linreg package that have already created previously call ridgereg(formula, data, lambda).As with the linreg() function it should take a formula object as well as a dataset and return a ridgereg
object. The ridgereg() function should also have the argument lambda to specify λ.

Ridge regression can be a good alternative when we have a lot of covariates (when p > n) or in the situation of multicollinearity.The hyperparameter that we will tune to find the best model is the λ parameters. Unlike the linear regression situation, different scalings of the covariates in X will affect the results. So normalize all covariates before you do the analysis.

$$Xnorm = x − x̄/√(V (x) ) \tag{7}$$
If you want to compare the results you can compare with lm.ridge() in the MASS package that is parametrized in the same way. But it uses SVD decomposition so there can be small differences in the results.

##  Computations using least squares

The simple way to calculate the different coefficients and values is to use ordinary linear algebra and
calculate:

Regressions coefficients:$$β ̂ridge=(X^T X+λI )^-1 X^T y \tag{8}$$

The fitted values: $$y ̂=Xβ ̂ridge \tag{9}$$

## Implementing methods

As with the linreg() function you should implement some methods for your object.

data(iris)
mod_object <- lm(Petal.Length~Species, data = iris)
print(mod_object)


Call:
lm(formula = Petal.Length ~ Species, data = iris)
Coefficients:
(Intercept) Speciesversicolor Speciesvirginica
1.46 2.80 4.09

predict() should return the predicted values yˆ, it should be able to predict for new dataset similar to the predict() function for the lm()

package.coef() should return the ridge regression coefficients βˆridge


## Handling large datasets with dplyr

Create a function call visualize airport delays() without any arguments that creates a plot that visualizes the mean delay of flights for different airports by longitude and latitude using ggplot2. The datasets can be found in the nycflights13 package.

## Create a vignettes for ridgereg(), dplyr and the caret package

Create a vignette called ridgereg where you show how to do a simple prediction problem using your own ridgereg() function.
Use the caret package and your ridgereg() function to create a predictive model for the BostonHousing data found in the mlbench package or (*) data from your own API. If you prefer you may use the tidymodels package instead of caret.The vignette should include the following:

1. Divide the BostonHousing data (or your own API data) into a test and training dataset using the caret package.

2. Fit a linear regression model and a fit a linear regression model with forward selection of covariates on the training dataset.

i. You may do this by using any R package that is available on CRAN, but you may wish to
use the caret package; see here
(https://topepo.github.io/caret/train-models-by-tag.html#Linear Regression)
ii. Remember that when using the forward selection algorithm on a dataset containing n
covariates, the algorithm should be able to select amongst models trained on 0 (only
intercept model), 1 (single covariate model), ..., n (model that uses all covariates, i.e. no
removal of covariates) covariates.
iii. If you do use caret::train for this task, make sure to check that the condition mentioned
in hint (b) holds. You could check this, for example, by calling the summary function on
output of the caret::train command; if this condition does not hold, check if tuning
any parameter(s) of caret::train helps.
iv. See here (https://topepo.github.io/caret/model-training-and-tuning.html#grids)
for help on how to tune caret::train parameters.
3. Evaluate the performance of this model on the training dataset.
4. Fit a ridge regression model using your ridgereg() function to the training dataset for different
values of λ. How to include custom models in caret is described here
https://topepo.github.io/caret/using-your-own-model-in-train.html .
5. Find the best hyperparameter value for λ using 10-fold cross-validation on the training set. More
information how to use the caret package for training can be found here
https://cran.r-project.org/web/packages/caret/vignettes/caret.html and here
https://topepo.github.io/caret/model-training-and-tuning.html.
6. Evaluate the performance of all three models on the test dataset and write some concluding comments.

## (*) Predictive modeling of flight delays using ridgereg()

Create a new vignette called flight delay where you try to predict the delay of each flight using your own
ridgereg() function. If the data is too large, you can scale it down a bit, but the purpose is to try to
do predictions using larger datasets.
1. Read in the weather dataset and the flights dataset from the nycflights13 package and remove
eventual variables you do not believe to have a predictive value.
2. Add extra weather data from the weather dataset and create interaction effects you think could be
of interest for the prediction.
3. Use the caret package to divide the flight dataset into three sets: test, train and validation (with
the proportions 5%, 80% and 15%.
4. Train ridge regressions models for different values of λ and evaluate the root mean squared error (see
here https://heuristically.wordpress.com/2013/07/12/calculate-rmse-and-mae-in-r-and-sas/)
on the validation set. Try to find an optimal value for λ.
5. When you found a good value for λ, use this to predict the test set and report the RMSE of your
predicted model.


